{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655fa5b6",
   "metadata": {},
   "source": [
    "# Machine Learning 2 \n",
    "## Course Project: Building an image classifier\n",
    "\n",
    "For this project, we will work on the __Swissroads__ data set which contains several hundreds images of vehicles found in the EPFL - Lausanne area including __cars, trucks, vans, bikes, motorcycles and others.__\n",
    "\n",
    "The goal of this project is to __test the different classifiers__ and techniques from the course using high-level features extracted with a __pretrained__ convolutional neural network from __TensorFlow Hub__ and compare the results with my own ConvNet implementation trained from the raw image pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a506a0a8",
   "metadata": {},
   "source": [
    "Let's start by importing the data. As we have folders and subfolders of images, we can use the __Keras Image Generator__ to import our images.\n",
    "\n",
    "### Keras Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f807a4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 images belonging to 6 classes.\n",
      "Found 139 images belonging to 6 classes.\n",
      "Found 50 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "# Create image generator\n",
    "image_generator = ImageDataGenerator(rescale=1/255) #Here we don't use extra transformation parameters because we have a validation set for this.\n",
    "\n",
    "# Create an iterator that iterates over the directory\n",
    "\n",
    "# Train, validation and test sets\n",
    "trainset = image_generator.flow_from_directory(\n",
    "    os.path.join('swissroads', 'train'), target_size=(299, 299),\n",
    "    shuffle=True, class_mode = 'sparse', batch_size = 280)\n",
    "validset = image_generator.flow_from_directory(\n",
    "    os.path.join('swissroads', 'valid'), target_size=(299, 299),\n",
    "    shuffle=False, class_mode = 'sparse', batch_size = 280)\n",
    "testset = image_generator.flow_from_directory(\n",
    "    os.path.join('swissroads', 'test'), target_size=(299, 299),\n",
    "    shuffle=False, class_mode = 'sparse', batch_size = 280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd259ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bike': 0, 'car': 1, 'motorcycle': 2, 'other': 3, 'truck': 4, 'van': 5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the data don't intersect with each other\n",
    "print(set(trainset.filenames).intersection(set(validset.filenames)))\n",
    "\n",
    "# Check the names of the labels\n",
    "trainset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4e323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: (280, 299, 299, 3)\n",
      "Train labels: (280,)\n",
      "Valid images: (139, 299, 299, 3)\n",
      "Valid labels: (139,)\n",
      "Test images: (50, 299, 299, 3)\n",
      "Test labels: (50,)\n"
     ]
    }
   ],
   "source": [
    "# Get all the images with their labels\n",
    "X_tr, y_tr = trainset.next()\n",
    "X_va, y_va = validset.next()\n",
    "X_te, y_te = testset.next()\n",
    "\n",
    "print('Train images:', X_tr.shape) \n",
    "print('Train labels:', y_tr.shape) \n",
    "print('Valid images:', X_va.shape) \n",
    "print('Valid labels:', y_va.shape) \n",
    "print('Test images:', X_te.shape) \n",
    "print('Test labels:', y_te.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa83b90",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "We know that CNN's alone (with no dense layers) don't do anything other than map the presence of features from our input. This means we could use a pretrained CNN, one trained on millions of images, as the start of our model. This would allow us to have a very good convolutional base before adding our own dense layered classifier at the end. We're lucky, some expert already did this and made their results publicly available. we will thus use the pre-trained model of the [Inception v3](https://tfhub.dev/google/imagenet/inception_v3/classification/5) ConvNets to classify our images. This will greatly help us because the convnet already has a very good idea of what features to look for in an image and can find them very effectively. So, if we can determine the presence of features all the rest of the model needs to do is determine which combination of features makes a specific image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f75ed4",
   "metadata": {},
   "source": [
    "### Create a graph with the inception V3 model\n",
    "\n",
    "Now that we have imported our images, let's extract the high-level features with the inception V3  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906d8d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Create graph\n",
    "img_graph = tf.Graph()\n",
    "\n",
    "with img_graph.as_default():\n",
    "    # Download module\n",
    "    module_url = 'https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1'\n",
    "    feature_extractor = hub.Module(module_url)\n",
    "\n",
    "    # Create input placeholder\n",
    "    input_imgs = tf.placeholder(dtype=tf.float32, shape=[None, 299, 299, 3])\n",
    "\n",
    "    # A node with the features\n",
    "    imgs_features = feature_extractor(input_imgs)\n",
    "\n",
    "    # Collect initializers\n",
    "    init_op = tf.group([\n",
    "        tf.global_variables_initializer(), tf.tables_initializer()\n",
    "    ])\n",
    "\n",
    "img_graph.finalize() # Good practice: make the graph \"read-only\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab19448",
   "metadata": {},
   "source": [
    "We created a computational graph called ```img_graph``` that contains our Inception V3 module. Let's now create a session with this graph, run initialization and pass our train images through the graph to get the imgs_features out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b7a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 2048)\n",
      "(139, 2048)\n",
      "(50, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Create a session\n",
    "sess = tf.Session(graph=img_graph)\n",
    "\n",
    "# Initialize it\n",
    "sess.run(init_op)\n",
    "\n",
    "# Extract features for train, validation and test set\n",
    "features = sess.run(imgs_features, feed_dict={input_imgs: X_tr})\n",
    "features_va = sess.run(imgs_features, feed_dict={input_imgs: X_va})\n",
    "features_te = sess.run(imgs_features, feed_dict={input_imgs: X_te})\n",
    "\n",
    "print(features.shape)\n",
    "print(features_va.shape)\n",
    "print(features_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66a89a",
   "metadata": {},
   "source": [
    "Rescale the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f6ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features = scaler.fit_transform(features)\n",
    "features_va = scaler.transform(features_va)\n",
    "features_te = scaler.transform(features_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b452cac",
   "metadata": {},
   "source": [
    "We have now the 2048 high-level features of our 280 train images. Let's store them in an npz file for future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0c8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Store the data in a npz file for loading more easily in further notebooks\n",
    "np.savez(\"processed_data/features_tr\", x=features, y=y_tr)\n",
    "np.savez(\"processed_data/features_va\", x=features_va, y=y_va)\n",
    "np.savez(\"processed_data/features_te\", x=features_te, y=y_te)\n",
    "\n",
    "np.savez(\"processed_data/train_data\", x=X_tr, y=y_tr)\n",
    "np.savez(\"processed_data/validation_data\", x=X_va, y=y_va)\n",
    "np.savez(\"processed_data/test_data\", x=X_te, y=y_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
